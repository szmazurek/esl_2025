{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb26b30",
   "metadata": {},
   "source": [
    "# Lab 3 & 4 - DNN optimization: quantization & pruning in PyTorch\n",
    "\n",
    "In our second lab, we will focus on two main techniques for optimizing deep neural networks: quantization and pruning. We will use the `torchao` library to implement these techniques and evaluate their impact on model performance and size. Both of these techniques are crucial for deploying models on resource-constrained devices, such as mobile phones and embedded systems (especially when using FPGAs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04773563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "class ToyLinearModel(torch.nn.Module):\n",
    "    def __init__(self, m: int, n: int, k: int):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(m, n, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(n, k, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    def example_inputs(self, batch_size=1, dtype=torch.float32, device=\"cpu\"):\n",
    "        return (\n",
    "            torch.randn(\n",
    "                batch_size, self.linear1.in_features, dtype=dtype, device=device\n",
    "            ),\n",
    "        )\n",
    "\n",
    "model = ToyLinearModel(1024, 1024, 1024).eval().to(torch.float16)\n",
    "model_fp16 = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57440624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO profile initial model (hint: use torch.profiler we saw in the previous lab)\n",
    "# You may also define it as a function to be able to reuse it later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44de0aa",
   "metadata": {},
   "source": [
    "## Static Quantization\n",
    "\n",
    "We can directly quantize a pre-trained model using static quantization. We can do that both with weights and activations. While in static quantization we can rather easily quantize weights, activations are more tricky. To accurately quantize activations, we almost always need to perform calibration, which involves running the model on a representative dataset to determine the optimal scaling factors (zero point, scaling factor) for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchao.quantization import Int8WeightOnlyConfig, quantize_\n",
    "# THIS IS INPLACE OPERATION!\n",
    "quantize_(model, Int8WeightOnlyConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04517fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "torch.save(model, \"/tmp/int8_model.pt\")\n",
    "torch.save(model_fp16, \"/tmp/float16_model.pt\")\n",
    "int8_model_size_mb = os.path.getsize(\"/tmp/int8_model.pt\") / 1024 / 1024\n",
    "float16_model_size_mb = os.path.getsize(\"/tmp/float16_model.pt\") / 1024 / 1024\n",
    "\n",
    "print(\"int8 model size: %.2f MB\" % int8_model_size_mb)\n",
    "\n",
    "print(\"float16 model size: %.2f MB\" % float16_model_size_mb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchao.utils import (\n",
    "    benchmark_model,\n",
    ")\n",
    "\n",
    "num_runs = 100\n",
    "torch._dynamo.reset()\n",
    "example_inputs = (torch.randn(8, 1024, dtype=torch.float16, device=\"cpu\"),)\n",
    "model(*example_inputs)  # warmup\n",
    "model_fp16(*example_inputs)  # warmup\n",
    "fp16_time = benchmark_model(model_fp16, num_runs, example_inputs)\n",
    "int8_time = benchmark_model(model, num_runs, example_inputs)\n",
    "\n",
    "print(\"fp16 mean time: %0.3f ms\" % fp16_time)\n",
    "print(\"int8 mean time: %0.3f ms\" % int8_time)\n",
    "print(\"speedup: %0.1fx\" % (fp16_time / int8_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc76eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO profile quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc7ff1",
   "metadata": {},
   "source": [
    "## Calibration \n",
    "\n",
    "To increase the accuracy of our quantization, we will perform calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fcb44",
   "metadata": {},
   "source": [
    "### Some utilities for post-training static quantization with calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ef946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torchao.quantization.quant_api import _replace_with_custom_fn_if_matches_filter\n",
    "\n",
    "class ObservedLinear(torch.nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        act_obs: torch.nn.Module,\n",
    "        weight_obs: torch.nn.Module,\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.act_obs = act_obs\n",
    "        self.weight_obs = weight_obs\n",
    "\n",
    "    def forward(self, input: Tensor):\n",
    "        observed_input = self.act_obs(input)\n",
    "        observed_weight = self.weight_obs(self.weight)\n",
    "        return F.linear(observed_input, observed_weight, self.bias)\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, float_linear, act_obs, weight_obs):\n",
    "        observed_linear = cls(\n",
    "            float_linear.in_features,\n",
    "            float_linear.out_features,\n",
    "            act_obs,\n",
    "            weight_obs,\n",
    "            False,\n",
    "            device=float_linear.weight.device,\n",
    "            dtype=float_linear.weight.dtype,\n",
    "        )\n",
    "        observed_linear.weight = float_linear.weight\n",
    "        observed_linear.bias = float_linear.bias\n",
    "        return observed_linear\n",
    "    \n",
    "\n",
    "\n",
    "def insert_observers_(model, act_obs, weight_obs):\n",
    "    _is_linear = lambda m, fqn: isinstance(m, torch.nn.Linear)\n",
    "\n",
    "    def replacement_fn(m):\n",
    "        copied_act_obs = copy.deepcopy(act_obs)\n",
    "        copied_weight_obs = copy.deepcopy(weight_obs)\n",
    "        return ObservedLinear.from_float(m, copied_act_obs, copied_weight_obs)\n",
    "\n",
    "    _replace_with_custom_fn_if_matches_filter(model, replacement_fn, _is_linear)\n",
    "\n",
    "\n",
    "is_observed_linear = lambda m, fqn: isinstance(m, ObservedLinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efb3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchao.quantization.quant_primitives import (\n",
    "    MappingType,\n",
    ")\n",
    "from torchao.quantization.observer import (\n",
    "    AffineQuantizedMinMaxObserver,\n",
    ")\n",
    "from torchao.quantization.granularity import (\n",
    "    PerAxis,\n",
    "    PerTensor,\n",
    ")\n",
    "\n",
    "mapping_type = MappingType.ASYMMETRIC\n",
    "target_dtype = torch.int8\n",
    "\n",
    "act_obs = AffineQuantizedMinMaxObserver(\n",
    "    mapping_type,\n",
    "    target_dtype,\n",
    "    granularity=PerTensor(),\n",
    "    eps=torch.finfo(torch.float32).eps,\n",
    "    scale_dtype=torch.float32,\n",
    "    zero_point_dtype=torch.float32,\n",
    ")\n",
    "weight_obs = AffineQuantizedMinMaxObserver(\n",
    "    mapping_type,\n",
    "    target_dtype,\n",
    "    granularity=PerAxis(axis=0),\n",
    "    eps=torch.finfo(torch.float32).eps,\n",
    "    scale_dtype=torch.float32,\n",
    "    zero_point_dtype=torch.float32,\n",
    ")\n",
    "insert_observers_(model_fp16, act_obs, weight_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fc0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchao.quantization.transform_module import register_quantize_module_handler\n",
    "from torchao.core.config import AOBaseConfig\n",
    "from torchao.dtypes import to_affine_quantized_intx_static\n",
    "from torchao.quantization import to_linear_activation_quantized\n",
    "\n",
    "@dataclass\n",
    "class StaticQuantConfig(AOBaseConfig):\n",
    "    target_dtype: torch.dtype\n",
    "\n",
    "\n",
    "# converting observed linear module to linear module with quantzied weights (and quantized activations)\n",
    "# with tensor subclasses\n",
    "@register_quantize_module_handler(StaticQuantConfig)\n",
    "def _apply_static_quant_transform(\n",
    "    module: torch.nn.Module,\n",
    "    config: StaticQuantConfig,\n",
    "):\n",
    "    target_dtype = config.target_dtype\n",
    "    observed_linear = module\n",
    "\n",
    "    weight_scale, weight_zero_point = observed_linear.weight_obs.calculate_qparams()\n",
    "\n",
    "    def weight_quant_func(weight):\n",
    "        block_size = (1, weight.shape[1])\n",
    "        if target_dtype == torch.int8:\n",
    "            return to_affine_quantized_intx_static(\n",
    "                weight, weight_scale, weight_zero_point, block_size, target_dtype\n",
    "            )\n",
    "        raise ValueError(f\"Unsupported target dtype {target_dtype}\")\n",
    "\n",
    "    linear = torch.nn.Linear(\n",
    "        observed_linear.in_features,\n",
    "        observed_linear.out_features,\n",
    "        False,\n",
    "        device=observed_linear.weight.device,\n",
    "        dtype=observed_linear.weight.dtype,\n",
    "    )\n",
    "    linear.weight = observed_linear.weight\n",
    "    linear.bias = observed_linear.bias\n",
    "\n",
    "    linear.weight = torch.nn.Parameter(\n",
    "        weight_quant_func(linear.weight), requires_grad=False\n",
    "    )\n",
    "\n",
    "    # activation quantization\n",
    "    act_scale, act_zero_point = observed_linear.act_obs.calculate_qparams()\n",
    "    if target_dtype == torch.int8:\n",
    "        input_quant_func = lambda x: to_affine_quantized_intx_static(\n",
    "            x, act_scale, act_zero_point, x.shape, target_dtype\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported target dtype {target_dtype}\")\n",
    "    linear.weight = torch.nn.Parameter(\n",
    "        to_linear_activation_quantized(linear.weight, input_quant_func),\n",
    "        requires_grad=False,\n",
    "    )\n",
    "\n",
    "    return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089f253",
   "metadata": {},
   "source": [
    "#### Important! Reinitialize the initial model before running the calibrated quantization, as we have already quantized it in the previous step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04035e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchao.quantization.utils import compute_error\n",
    "torch.manual_seed(0)\n",
    "model_fp16 = ToyLinearModel(1024, 1024, 1024).eval().to(torch.float16)\n",
    "model_int8 = copy.deepcopy(model_fp16)\n",
    "example_inputs = model_fp16.example_inputs(batch_size=32, device=\"cpu\", dtype=torch.float16)\n",
    "outputs_fp16 = model_fp16(*example_inputs)\n",
    "insert_observers_(model_int8, act_obs, weight_obs)\n",
    "\n",
    "# Run some calibration data through the model to collect statistics\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        model_int8(*example_inputs)\n",
    "\n",
    "quantize_(model_int8, StaticQuantConfig(target_dtype=target_dtype), is_observed_linear)\n",
    "\n",
    "outputs_int8 = model_int8(*example_inputs)\n",
    "print(\"Output error post vs pre quantization (FP16 vs INT8): %.6f\" % compute_error(outputs_fp16, outputs_int8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffcbeb4",
   "metadata": {},
   "source": [
    "## TODO\n",
    "1. Based on previous lab, modify the our Toy model to perform classificaton on MNIST dataset (10 classes).\n",
    "2. Train the model for one epoch on MNIST dataset\n",
    "3. Evaluate the model on the test set and compute accuracy\n",
    "4. Perform static quantization with calibration\n",
    "5. Evaluate the model after quantization\n",
    "6. Compare the accuracies, latencies and model sizes before and after quantization\n",
    "7. You may try to work with compilation too to see if it helps with execution speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e981844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bc10f",
   "metadata": {},
   "source": [
    "## Extra TODO\n",
    "Look at the example of how to export a model into such a format that can be directly ran on embedded devices (e.g. microcontrollers).\n",
    "Take a look at [this tutorial](https://docs.pytorch.org/executorch/stable/using-executorch-export.html) to see how to export a model to the `.pte` format using the `executorch` library. Then, try to save our model that way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
