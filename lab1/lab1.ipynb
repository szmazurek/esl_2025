{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "008a008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab setup\n",
    "# !pip install torch torchvision torchmetrics torchinfo tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dee675",
   "metadata": {},
   "source": [
    "# Lab 1 & 2 - Deep Learning Basics and Efficient Architectures\n",
    "\n",
    "Welcome to the first lab(s) of the course! Throught the course, we will gradually learn about edge AI systems.\n",
    "In this lab, we will start with the basics of deep learning and PyTorch framework. We will learn how to create, train and evaluate neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc205c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8065935f",
   "metadata": {},
   "source": [
    "# The basics of PyTorch\n",
    "\n",
    "Pytorch is a popular deep learning framework that provides a flexible and efficient way to build and train neural networks. The core of Pytorch is it's automatic differentiation engine that allows us to compute gradients and perform backpropagation. This is essential for training neural networks using gradient descent. <br>\n",
    "\n",
    "At a high level, Pytorch is a fronted for tensor computations, similar to NumPy, but with additional capabilities for GPU acceleration and automatic differentiation. It's power lies in its simple and intuitive API, which makes it easy to build and experiment with complex neural network architectures using pre-built components written in lower level languages like C++ or CUDA.\n",
    "\n",
    "### Tensors - fundamental building blocks of PyTorch\n",
    "\n",
    "`Tensors` are the basic data structures in Pytorch, similar to numpy arrays, but with additional capabilities for GPU acceleration and automatic differentiation. In general, they are generalized data containers that can hold a data in arbitrary number of dimensions (even 0D or more than 3D).\n",
    "\n",
    "![Tensors](https://tinyurl.com/bdnxcym)\n",
    "\n",
    "### Basic properties of tensors\n",
    "\n",
    "In Pytorch, tensors have several important properties:\n",
    "- `shape` - the dimensions of the tensor, represented as a tuple of integers.\n",
    "- `dtype` - the data type of the elements in the tensor, such as `float32`, `int64`, etc.\n",
    "- `device` - the device on which the tensor is stored, such as `cpu` or `gpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fefb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 2x2 tensor directly from lists\n",
    "tensor = torch.tensor([[1, 2], [3, 4]]) \n",
    "print(\"Tensor:\\n\", tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4565611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eploring basic tensor properties\n",
    "print(\"Shape of tensor:\", tensor.shape)\n",
    "print(\"Data type of tensor:\", tensor.dtype)\n",
    "print(\"Device of tensor:\", tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an uninitialized 2x3 tensor, which may contain arbitrary values\n",
    "# which are already present at that memory location during allocation\n",
    "tensor_empty = torch.empty((2, 3))\n",
    "print(\"Empty Tensor:\\n\", tensor_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 3x3 tensor filled with zeros\n",
    "tensor_zeros = torch.zeros((3, 3))\n",
    "print(\"Zeros Tensor:\\n\", tensor_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 2x4 tensor filled with ones\n",
    "tensor_ones = torch.ones((2, 4))\n",
    "print(\"Ones Tensor:\\n\", tensor_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85eb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 3x3 tensor with random values from a uniform distribution over [0, 1)\n",
    "tensor_random = torch.rand((3, 3))\n",
    "print(\"Random Tensor:\\n\", tensor_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43011f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 2x2 tensor from a NumPy array\n",
    "np_array = np.array([[5, 6], [7, 8]])\n",
    "tensor_from_np = torch.from_numpy(np_array)\n",
    "print(\"Tensor from NumPy array:\\n\", tensor_from_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting a PyTorch tensor to a NumPy array\n",
    "tensor_to_np = tensor.numpy()\n",
    "print(\"NumPy array from Tensor:\\n\", tensor_to_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26dc4c3",
   "metadata": {},
   "source": [
    "## Tensor access, manipulation and reshaping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand((4, 4))\n",
    "print(\"Original Tensor:\\n\", tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4164ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing elements\n",
    "print(\"Element at (0, 0):\", tensor[0, 0])\n",
    "print(\"First row:\", tensor[0, :])\n",
    "print(\"Second column:\", tensor[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94947a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying elements\n",
    "tensor[0, 0] = 0\n",
    "print(\"Modified Tensor:\\n\", tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping tensors\n",
    "reshaped_tensor = tensor.view(2, 8)\n",
    "print(\"Reshaped Tensor:\\n\", reshaped_tensor)\n",
    "\n",
    "# Reshaping (other way)\n",
    "reshaped_tensor_2 = tensor.reshape(2, 8)\n",
    "print(\"Reshaped Tensor (other way):\\n\", reshaped_tensor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea1ec7",
   "metadata": {},
   "source": [
    "#### Question - what is the difference between `view` and `reshape` methods in PyTorch? When would you use one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc268ea8",
   "metadata": {},
   "source": [
    "### Data types and devices\n",
    "\n",
    "Tensors generally contain data of the same type (i.e `float32`, `int64`, etc.). This is important, as it will influence the memory usage and may speed up executed arithmetic operations. Torch defines several data types, with the most common being `torch.float32` and `torch.int64`. It also supports half-precision (16-bit) and bfloat16 types, which are useful for reducing memory usage and speeding up computations on compatible hardware. <br>\n",
    "\n",
    "Additionally, tensor objects interface with different hardware devices, such as CPUs and GPUs. By calling specific methods, we can move tensors between memory on different devices. Note that moving tensors between devices can be time-consuming, so it's important to minimize the number of device transfers during training and inference. Operations between tensors on different devices are not allowed, so we need to ensure that all tensors involved in a computation are on the same device.\n",
    "\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/gpu-devotes-more-transistors-to-data-processing.png\" alt=\"Jupyter Logo\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of a tensor\n",
    "tensor_int = tensor.to(torch.int32)\n",
    "print(\"Tensor with int32 data type:\\n\", tensor_int)\n",
    "print(\"Data type of new tensor:\", tensor_int.dtype)\n",
    "\n",
    "# Move tensor to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor_gpu = tensor.to('cuda')\n",
    "    print(\"Device of tensor on GPU:\", tensor_gpu.device)\n",
    "\n",
    "    # Move tensor back to CPU\n",
    "    tensor_cpu = tensor_gpu.to('cpu')\n",
    "    print(\"Device of tensor back on CPU:\", tensor_cpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1662607",
   "metadata": {},
   "source": [
    "# nn.Module - building blocks for neural networks\n",
    "\n",
    "Pytorch provides a high-level API for building neural networks using the `nn.Module` class. This class provides a convenient way to define and organize the layers of a neural network, as well as to manage the parameters of the network. In general, network layers are defined by their parameters (weights, biases, convolutional kernels, etc.) and the operations they perform on the input data (linear transformations, non-linear activations, pooling, etc.).\n",
    "Let's see how to create a simple feedforward neural network using `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)  \n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc157c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.rand((1, 10))\n",
    "output = model(example_input)\n",
    "print(\"Model output:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf62d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "summary(model, (1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look model weight matrices\n",
    "print(\"Weights of fc1 layer:\\n\", model.fc1.weight)\n",
    "print(\"Weights of fc2 layer:\\n\", model.fc2.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751034a5",
   "metadata": {},
   "source": [
    "## Model size, number of parameters and other memory requirements\n",
    "\n",
    "The neural network model size is determined by the number of parameters (weights and biases) it contains. Each parameter typically requires a certain amount of memory, depending on its data type (e.g., float32, float64, etc.). The total memory required for a model can be calculated by multiplying the number of parameters by the size of each parameter in bytes. Considering already trained model, there is still a need to store more than parameters:\n",
    "\n",
    "- Activations - intermediate outputs of each layer during the forward pass\n",
    "- Operation instructions (kernels) - the operations that need to be performed during the forward pass (like matrix multiplications, convolutions, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure parameter size of the model\n",
    "\n",
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "\n",
    "# There are some params that are not trainable, but still take space and change during training\n",
    "# e.g. running mean and variance in BatchNorm layers.\n",
    "\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_b = (param_size + buffer_size)\n",
    "print('model size: {:.0f}B'.format(size_all_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1d1ea",
   "metadata": {},
   "source": [
    "## Measuring model inference latency\n",
    "\n",
    "To measure the inference latency of a neural network model, we can use the Pytorch native Timer module. This module provides a simple way to measure the time taken to execute a set of operations. It takes into account such things as GPU warm-up time or device synchronization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c57fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.benchmark import Timer\n",
    "timer = Timer(\n",
    "    stmt='model(example_input)',\n",
    "    globals={'model': model, 'example_input': example_input}\n",
    ")\n",
    "# Measure the time taken for 1000 inferences\n",
    "measurement = timer.timeit(1000)\n",
    "print(f\"Median inference time over 1000 runs: {measurement.median * 1e6:.3f} us\")\n",
    "print(f\"Mean inference time over 1000 runs: {measurement.mean * 1e6:.3f} us\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6d817",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders\n",
    "\n",
    "Pytorch provides a convenient way to handle datasets and data loading using the `torch.utils.data` module. This module provides two main classes: `Dataset` and `DataLoader`. The `Dataset` class is an abstract class that represents a dataset, while the `DataLoader` class is responsible for loading data from a dataset in batches.\n",
    "In the end, our input data ends up as tensors, which can be directly fed into our neural network. We can also leverage various pre-defined datasets available in `torchvision.datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# We will use the MNIST dataset of handwritten digits\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Visualize some samples from the training dataset\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 4))\n",
    "for i in range(3):\n",
    "    image, label = train_dataset[i]\n",
    "    ax[i].imshow(image.squeeze(), cmap='gray')\n",
    "    ax[i].set_title(f'Label: {label}')\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing DataLoaders - these will handle batching\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b57ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple CNN model for MNIST\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e40cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "def train(model, device):\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    acc_metric = Accuracy(\"multiclass\", num_classes=10).to(device)\n",
    "    num_epochs = 1\n",
    "    \n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), total=num_epochs, desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        epoch_preds = []\n",
    "        epoch_labels = []\n",
    "        for batch in tqdm(train_loader, total=len(train_loader)):\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_preds.append(outputs)\n",
    "            epoch_labels.append(labels)\n",
    "        epoch_preds = torch.cat(epoch_preds)\n",
    "        epoch_labels = torch.cat(epoch_labels)\n",
    "        train_acc = acc_metric(epoch_preds, epoch_labels)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n",
    "train(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6023f3e4",
   "metadata": {},
   "source": [
    "## Increasing the performance of our model\n",
    "\n",
    "We have used really simple architecture for our model. Currently, vision models have made a huge progress and they are able to achieve really high accuracy on various tasks. However, we are interested not only in the model predictive performance, but also in its efficiency. The field of TinyML is focused on creating models that can run on edge devices with limited computational resources. We will explore various techniques to improve the efficiency of our models, but for now we will try to start with some more advanced architecture that is designed with efficiency in mind.\n",
    "\n",
    "That being said, we would need some concrete metricts to evaluate not only the performance of our model on given task, but also its computational efficiency. We will use the following metrics, beyond the previously mentioned ones:\n",
    "- memory footprint - the amount of memory required to store th model parameters and intermediate activations during inference\n",
    "- number of operations (FLOPs) - the total number of floating-point operations required to perform a forward pass through the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92032898",
   "metadata": {},
   "source": [
    "### FLOPs - Floating Point Operations, a measure of computational complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76615e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to count model FLOPs - kudos https://alessiodevoto.github.io/Compute-Flops-with-Pytorch-built-in-flops-counter/\n",
    "\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from typing import Union, Tuple\n",
    "def get_flops(model, inp: Union[torch.Tensor, Tuple], with_backward=False):\n",
    "    \n",
    "    istrain = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    inp = inp if isinstance(inp, torch.Tensor) else torch.randn(inp)\n",
    "\n",
    "    flop_counter = FlopCounterMode(mods=model, display=False, depth=None)\n",
    "    with flop_counter:\n",
    "        if with_backward:\n",
    "            model(inp).sum().backward()\n",
    "        else:\n",
    "            model(inp)\n",
    "    total_flops =  flop_counter.get_total_flops()\n",
    "    if istrain:\n",
    "        model.train()\n",
    "    return total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "flops = get_flops(model, (1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69feaa87",
   "metadata": {},
   "source": [
    "### Memory footprint - the amount of memory required to store the model parameters and intermediate activations during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.profiler\n",
    "from torch.profiler import ProfilerActivity\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "with torch.profiler.profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True) as prof:\n",
    "    model(torch.randn((1, 1, 28, 28)).to(device))\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f62fa2",
   "metadata": {},
   "source": [
    "## TODO: fine-tune `MobilenetV3` on CIFAR-10 dataset and evaluate its performance using the defined metrics.\n",
    "\n",
    "Now let's try to use the knowledge we have gained so far and fine-tune `MobilenetV3` on CIFAR-10 dataset. We will also evaluate its performance using the defined metrics.\n",
    "\n",
    "1. Load the CIFAR-10 dataset using `torchvision.datasets` module and create a `DataLoader` for training and validation sets.\n",
    "2. Create the training loop.\n",
    "3. Train and evaluate the model.\n",
    "4. Measure the model size, number of parameters, FLOPs, total memory footprint and inference latency.\n",
    "\n",
    "Hint - do we really need to train the whole model from scratch? Maybe we can leverage some pre-trained weights? Do we need to train all the layers? Maybe we can freeze some of them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605b4f2",
   "metadata": {},
   "source": [
    "### EfficientNet architecture \n",
    "\n",
    "EfficientNet model family and subsequent versions introduce some key architectural innovations that contribute to their efficiency and performance:\n",
    "- Compound Scaling - EfficientNet uses a compound scaling method that uniformly scales the depth, width, and resolution of the input image. This allows the model to achieve better performance with fewer parameters and FLOPs.\n",
    "- MBConv Blocks - These convolutional blocks works as kind of inverted residuals blocks, where the input is \n",
    "\n",
    "1. Expanded to a higher dimensional space using a pointwise (1x1) convolution (increase feature representation space by increasing number of channels)\n",
    "2. Then, a depthwise convolution is applied to each channel separately to capture spatial information\n",
    "3. Non-linear activation function is applied (ReLU6 or HardSwish, also computationnally efficient)\n",
    "4. Next, input is once again passed through 1x1 convolution to reduce the number of channels back to the original size\n",
    "5. Finally, a skip connection is added to the output of the block to help with gradient flow during training.\n",
    "\n",
    "Sometimes, also a squeeze-and-excitation (SE) block is added after the depthwise convolution. In this block:\n",
    "\n",
    "1. The input is globally averaged pooled to create a channel-wise descriptor, which captures the global information carried by each channel\n",
    "2. This descriptor is then passed through a small fully connected network to learn channel-wise weights, kind of attention mechanism\n",
    "3. Finally, the input is scaled (multiplied) by these weights to recalibrate the channel-wise feature responses.\n",
    "\n",
    "You can use `torchinfo` to see a list of all layers in the model - take a look and see if you can find those elements in the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v3_small\n",
    "model = mobilenet_v3_small(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
